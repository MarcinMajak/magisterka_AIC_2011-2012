\section{Pattern recognition algorithms}
\label{cha:Introduction}
\subsection{Introduction}
Pattern recognition is a wide area of science in which we are interested in
assigning label from a given set of classes to every unknown pattern. The whole 
process of classification can be divided into few phases:
\begin{enumerate}
    \item Data collection
    \item Feature selection
    \item Model selection
    \item Classifier selection
    \item Training 
    \item Testing
\end{enumerate}
At first in this section few basic information and categories will be given 
about pattern recognition. Generally, the whole process of classification
can be broken down into two main categories
\begin{itemize}
    \item supervised - incoming to the system objects are not previously
        labeled and this is the system task to find an appropriate structure of
        the data, to establish the organization of the classes basing only on
        the available data, there is no statistical or expert knowledge at a
        hand.
    \item unsupervised - in this approach incoming patterns have labels and can
        be treated as a training set. This allows the classifier can retrieve
        information from data 
\end{itemize}
In this thesis supervised learning is reconsidered because available datasets 
are labeled with class number. When we take into account applied approach
pattern recognition, we can distinguish syntactic and statistical 
pattern recognition. In former each pattern is represented in terms of 
$d$ features, measurements and is viewed as a point in
$d$-dimensional feature space, while the latter
is  based on the characterization of the inherent structure of the 
qualitative features. For that  reason, the complex patterns can be
decomposed using a hierarchical structure in simple  sub-patterns.
The patterns are viewed as sentences belonging to a language, primitives
represents the alphabet of the language and the sentences are generated
according to a grammar which is inferred from the available training data.
EKG waveforms, textured images and shape analysis of contours are the examples
of syntactic approach.

\subsection{Problem statement}
\label{cha:Problem_statement}
In this section the problem statement will be presented in case of pattern
recognition. For the purpose of this thesis we assume supervised learning and
denote each pattern by the label $j \in M$, where $M$ is an $m$-element set of
possible states numbered with the successive natural numbers. The state $j$ is
unknown and does not undergo the direct investigation. What can only be
measured are attributes or features by which a state manifests itself. Each
object will be described by a $d$-dimensional measured feature vector $x \in
X$. In order to classify unknown pattern we use knowledge stored in training
set consisting of $N$ training patterns
$$S = (x_1, j_1), (x_2, j_2), \ldots, (x_N, j_N)$$
In practice the decision with learning should use knowledge included in the
training set $S$ and as the consequence the algorithm with learning is of the
following form:
$$i=\Psi(S, x), \, i \in M$$
In decision theory, to ensure that $\Psi$ approximate the problem as closely
as possible an additional loss function is introduced that assigns a specific
value to loss resulting from producing an incorrect label. The particular loss
function depends on the type of label being predicted. In case of
classification problem it is zero-one loss function. This corresponds simply to
assigning a loss of 1 to any incorrect labeling and is equivalent to computing
the accuracy of classification procedure over the set of training data.

\subsection{Rough sets}
\label{cha:Rough_set}
\subsubsection{Introduction}
\label{cha:Rough_set_introduction}
Rough sets theory represents mathematical approach to deal with imperfect knowledge. 
In the standard approach we need precise information about pattern to recognize, 
while rough sets can deal with vague or incomplete data. The problem of imperfect 
information was tackled for a long time and it became a crucial issue for many scientist.
One of the most prominent approaches in the recent years are fuzzy logic and rough sets.
In this section the latter approach is presented in greater details. Comparing with 
other methods rough sets have many advantages, but one of the most important
one is that it works only on the raw data, no additional information are needed such 
as density probability in Bayesian algorithm. The main facts about rough sets can be 
summarized in few point presented below:
\begin{enumerate}
    \item provides attribute reduction
    \item generates set of easy to understand and readable decision
        \textit{IF-THEN} rules
    \item evaluate significance of data 
\end{enumerate}

When talking about rough set theory one has to understand the concept of a set 
and how a rough set is related to the classical set represented in mathematic.
From the mathematical point of view the crisp (precise) set is a collection of 
objects of interest and is uniquely determined by its elements. In other words,
it means that every element must be uniquely classified as belonging to the set 
or not (true or false). For example, the set of odd numbers is crisp because every
number is either odd or even and cannot be partially in both. 

The nature of problem we met is much more complicated than simple decision that
objects belong to the set or not. For some sets we cannot precisely describe element
membership. Reconsider the group of people and division into set of small and
high people. The height is not a precise but a vague concept and data vagueness can 
be met in many problems found in the nature. Here is the spot for rough sets
theory where vagueness is expressed by a boundary region of a set. 

\subsubsection{Basic notation}
\label{cha:Rough_set_basic_notation}
In rough sets theory to represent datasets (information) we introduce a notion 
called an \textit{information system}. It can be described by 4-tuple
$$IS = <U, Q, V, f >$$ 
\begin{itemize}
    \item $U$ is the universe of discourse  which is a finite set of objects
    \item $Q$ is a finite set of attribute by  which each patterns manifests itself
    \item $V = \bigcup V_q$, $V_q$ represents a domain of attribute $q$
    \item $f:U \times Q \rightarrow V$ is a total information function, such that
        $\bigvee_{q\in Q, x \in U} f(x,q) \in U$
\end{itemize} 
The information system can be represented as a finite table in which 
columns are labeled by attributes and each rows stands for an object from
$IS$. Over the information table we can define decision table
$T$ where the set of attributes $Q$ is disjoined into two
subset $C$ and $D$. The set $C$ is a subset of 
condition attributes, and the set $D$ contains decision attributes 
by which we can partition set $U$ into decision classes.

From the granular nature of rough sets it may happen that some objects 
in the $U$ are indistinguishable due to the limited information. Now, let
define an indiscernibility relation $R \rightarrow U \times U$, representing the 
lack of knowledge about patterns in the set $U$. The indiscernibility relation on
$U$ can be extended and associated with every non-empty subset of attributes $P \subseteq Q$
and is defined as follows 
$$I_p = \{ (x, y) \in U \times U: f(x, q) = f(y,q), \bigvee_{q \in P}\}$$ 
Now having $I_p$ we can say that objects $x$ and
$y$ are $P$-indiscernible by a set of attributes $P$ is $y \, I_p \, x$. Relation
$I_p$ divides the set $U$ into blocks (concepts) of $P$-indiscernible objects.
The $P$-elementary set containing objects $P$-indiscernible with $x \in U$ is
referred as $I_p(x)$ and defined as follows:
$$I_p = \left\{ y \in U: y \, I_p \, x \right\}$$

By representing a target concept $X$ as a subset of $U$ we would like to
describe it with respect to $R$. Additionally let introduce $P$ as non-empty
subset of attributes from $Q$. In rough sets reasoning object membership to a
set can be represented in two ways:
\begin{enumerate}
    \item An object $x \in U$ certainly belongs to $X$ if
        all objects from the $P$-elementary set defined by $I_p(x)$ also belong to $X$.
        A set of all objects certainly belonging to $X$ creates the $P$-lower
        approximation of $X$ and can be represented as follows:
            $$\underline{I_p} = \{ x \in U: I_p(x) \subseteq X\}$$
    \item An object $x \in U$ can possibly belong to $X$ if at least one object
        from $P$-elementary set $I_p(x)$ can possibly belong to $X$. All the
        objects that could possibly belong to $X$ are denoted as $P$-upper
        approximation of $X$, defined as:
        $$\overline{I_p} = \{x\in U: I_p(x) \, \cap \, X \neq \emptyset \}$$
        Therefore the set $U - \overline{I_p}$ represents the negative region,
        containing the set of objects that can be definitely ruled out as
        members of the target set.
\end{enumerate}
The tuple $<\underline{I_p}, \overline{I_p}>$ representing a lower boundary of
the target $X$ and the upper boundary of the target $X$ creates a rough set.
Using above notions we can define $P$-boundary region which is a difference
between upper and lower approximation. 
$$BN_p(X) = \overline{I_p} - \underline{I_p}$$
The $BN_p(X)$ is a set of elements which cannot be certainly classified neither
as $X$ nor as not-$X$ with respect to the set of attributes $P$. If the
boundary region of $X$ is empty then it is crisp, otherwise we deal with
inexact set which is called rough set. Until that moment we can see that 
rough sets concept can be defined quite generally by means of topological
operations: interior and closure, called approximations. They express the
knowledge about pattern in terms of granules, not by a precise measure.

\subsubsection{Properties of rough sets}
The same as classical sets or fuzzy logic rough sets can be described by the
following properties:
\begin{enumerate}
    \item $\overline{I_p} \subseteq \, X \, \subseteq \, \underline{I_p}$
    \item $\overline{I_p}(\emptyset) = \underline{I_p}(\emptyset) = \emptyset;
        \;
        \overline{I_p}(U) = \underline{I_p}(U) = U $
    \item $\overline{I_p}(X \cup Y) = \overline{I_p}(X) \cup \overline{I_p}(Y)$
    \item $\underline{I_p}(X \cap Y) = \underline{I_p}(X) \cap \underline{I_p}(Y)$
\end{enumerate}
% TODO Dokonczyc to

\subsubsection{Attribute reduction and dependency}
\label{cha:Rough_set_attribute_reduction}
% TODO Dokonczyc to



\subsubsection{Rough sets reasoning from data}
The category description can be done in two ways:
\begin{enumerate}
    \item extensional
    \item intentional
\end{enumerate}
To represent a concept we have to be able to identify 
all objects belonging to this category. 
With the former approach we have no insight 
into decision engine so we do not know how 
to assign new objects to the category. In 
the latter approach we represent the category 
based on the set of rules. The same approach 
is done in rough sets algorithm where an elementary 
granules (concepts) of knowledge build blocks consisting 
of indiscernible pattern from the universe of discourse. 
We will associate decision rules with decision table $T$.

In this section a practical example will be presented to clear all the things
out. As an example let reconsider well-known example of patients suffering from
flu. 
\begin{table}[H]
    \centering
    \caption{Example dataset showing healthy patients and suffering from flu}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline 
    Patient & Headache & Muscle pain & Temperature & Flu \\ \hline \hline
    p1 & no & yes & high & yes \\ \hline
    p2 & yes & no & high & yes \\ \hline
    p3 & yes & yes & very high & yes \\ \hline
    p4 & no & yes & normal & no \\ \hline
    p5 & yes & no & high & no \\ \hline
    p6 & no & yes & very high & yes \\ \hline    
    \end{tabular}
    \label{tab:example_rough_set}
\end{table}
Table \ref{tab:example_rough_set} represents an information system about
healthy patient and those suffering from flu. Attributes: Headache,
Muscle-pain, Temperature are called condition attributes, while the attribute
``Flu'' (last column in table \ref{tab:example_rough_set}) is considered as
decision attribute. Each row of a decision table determines a decision rule,
for example

IF HEADACHE IS 'NO' AND MUSCLE\_PAIN IS 'YES' AND TEMPERATURE IS 'HIGH' THEN FLU IS 'YES'

Here we can generate few indiscernible relations based on 
the chosen attributes. In case of Headache attribute patients p2, p3, p5 are indiscernible; patients p2, p5 are
indiscernible with respect to attributes Headache, Muscle-pain and Temperature. 
Using Headache and Muscle-Pain we can divide the set into three sets:
\{p1, p4, p6\}, \{p2, p5\}, \{p3\}.

Now it is time for defining key features of rough sets. Over the table
\ref{tab:example_rough_set} we can define two concepts: ``Flu'' and
``Not Flu''. For the first concept the lower approximation
set of patient certainly having flu is \{p1, p3, p6\}, while the upper
approximation of patients possibly suffering from flu is \{p1, p2, p3, p5,
p6\}. The boundary region for concept ``flu'' is a set of \{p2, p5\} patients. 
For the concept ``Not Flu'' the lower approximation is the set \{p4\}, whereas
the upper approximation is the set \{p2, p4, p5\}. Again the boundary region is
the set \{p2, p5\}.

Additionally, we can measure the accuracy of approximation $\alpha_B(x)$ for each concept.
This can tell us if the set of attributes to describe the concept is correctly
chosen. For the ``Flu'' concept where $X$=\{p1, p2, p3, p6\} is described by
set of attributes $B$=\{Headache, Muscle-pain, Temperature\} the accuracy of
approximation is $\alpha_B(Flu) = \frac{3}{5}$. On the other hand when we take
only one attribute $B$=\{Temperature\}, then we get lower approximation of \{p3,
p6\} and upper approximation of \{p1, p2, p3, p5, p6 \} resulting in
$\alpha_B(Flu) = \frac{2}{5}$. To sum up, $\alpha_B(x)$ is a very important indicator in
rough sets theory and tells which attributes better characterize target
concept. 


\subsection{Fuzzy logic}
\label{cha:Fuzzy_logic}
\subsubsection{Introduction}
\subsubsection{Fuzzy reasoning from data}
