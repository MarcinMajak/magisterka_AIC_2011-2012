\section{Experimentation system}
\label{cha:ExperimentAnalysis}
\subsection{Assumptions}
This section starts the second part of this thesis. In the first, algorithms
were presented in case of construction and their properties. For the simulation 
purposes the following algorithms have been implemented:
\begin{enumerate}
    \item Basic Rough sets algorithm
    \item Rough sets algorithm with modification of decision rules basing on
        the granulation step $G$
    \item Fuzzy logic algorithm with genetic approach for rule construction
    \item Hybrid algorithm consisting of:
        \begin{itemize}
            \item Rough sets algorithm with genetic approach for finding the
                optimal partition in feature space and attribute reduction 
            \item Fuzzy logic algorithm with genetic approach for constructing
                decision rules
        \end{itemize}
\end{enumerate}

\subsection{Datasets}
\label{cha:Datasets}
To perform all the experiments, testing datasets from \textit{UCI} repository were used \cite{bib47}.
This approach is commonly used in the literature because ensures that someone
in the future would be able to retake the tests and compare the results. 
The main motivation in choosing those datasets was to ensure diversity and test
algorithms with complicated and complex problems. Below each dataset is shortly
described:
\begin{itemize}
    \item Dataset name: Haberman
        \begin{itemize} 
            \item \#attributes: 3
            \item \#instances: 306
            \item \#classes: 2
            \item Desctiption:
                This dataset contains cases from a study that was conducted
                between 1958 and 1970 at the University of Chicago's Billings Hospital 
                on the survival of patients who had undergone surgery for breast cancer.
        \end{itemize}
    \item Dataset name: Iris
        \begin{itemize}
            \item \#attributes: 4
            \item \#instances: 150
            \item \#classes: 3
            \item Desctiption:
                This dataset is one the most commonly used in
                pattern recognition task. Attribute information:
                \begin{itemize}
                    \item sepal length in cm 
                    \item sepal width in cm 
                    \item petal length in cm 
                    \item petal width in cm 
                \end{itemize}
        \end{itemize} 
    \item Dataset name: Wine
        \begin{itemize}
            \item \#attributes: 13
            \item \#instances: 178
            \item \#classes: 3
            \item Desctiption:
                These data are the results of a chemical analysis of wines
                grown in the same region in Italy but derived from three 
                different cultivars. The analysis determined the quantities 
                of 13 constituents found in each of the three types of wines. 
        \end{itemize}

    \item Dataset name: Thyroid
        \begin{itemize}
            \item \#attributes: 5
            \item \#instances: 215
            \item \#classes: 3
            \item Desctiption:
                This dataset was created at the University of
                California at Irvine by Ross Quinlan during his visit in 1987 for
                the 1987 Machine Learning Workshop. It contains 5 features
                describing thyroid symbptoms.
        \end{itemize}

    \item Dataset name: Bupa
        \begin{itemize}
            \item \#attributes: 6
            \item \#instances: 345
            \item \#classes: 2
            \item Desctiption:
                The first 5 variables are all blood tests which are thought to
                be sensitive to liver disorders that might arise from excessive 
                alcohol consumption. Each line in the bupa.data file constitutes
                the record of a single male individual. 
        \end{itemize}

    \item Dataset name: Pima
        \begin{itemize}
            \item \#attributes: 8
            \item \#instances: 768
            \item \#classes: 2
            \item Desctiption:
                This dataset comes from National Institute of Diabetes and Digestive and
                Kidney Diseases. The binary-valued decision indicates whether the
                patient shows signs of diabetes according to World Health Organization
                criteria. All patients are females at least 21 years old of Pima Indian heritage.
        \end{itemize}

    \item Dataset name: Wdbc
        \begin{itemize}
            \item \#attributes: 32
            \item \#instances: 569
            \item \#classes: 2
            \item Desctiption:
                Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.
                Each real-valued features are computed for each cell nucleus.
        \end{itemize}
\end{itemize}
\subsection{Efficiency indicators}
\label{cha:indicators}
To evaluate effectiveness of algorithm it has to be consistent approach used in
all experiments and additionally a~priori knowledge about each dataset must
be know. By a-priori knowledge one should understand the label of class for
each pattern. Below, there are listed methods of algorithm fitness scoring:
\begin{itemize}
	\item Classification accuracy $CA$- the number of correctly classified
        patterns out of $O$ objects.
		\begin{itemize}
			\item The best value from $n$ probes 
				\begin{equation}
                    B=max\{CA_1, \ldots, CA_n\}
					\label{min1}
				\end{equation}
			\item The worst value from $n$ probes 
				\begin{equation}
                    W=min\{CA_1, \ldots, CA_n\}
					\label{min3}
				\end{equation}
			\item The average value from $n$ probes
				\begin{equation}
                    A=\frac{1}{n}\cdot\sum\limits_{i=1}^{n} (CA_i)
					\label{min2}
				\end{equation}
		\end{itemize}
	\item Error variance from $n$ simulations 
		\begin{equation}
            \sigma=\sqrt{\frac{1}{n-1}\sum\limits_{i=1}^n(AC_i-A)^2}
			\label{min4}
		\end{equation}
\end{itemize}
\subsection{Program description}
%-----------------------------------------------------------------------------------------
To perform all simulations in this project a program was written in
\textit{PYTHON}(more information can be found in \ref{Appendix}). 
It allows simulating all algorithms with chosen dataset and obtain
algorithm efficiency indicators. 

Program was tested on Linux platform with Intel Pentium Dual Core 2.4 GHz, 2GB
memory. To run the program one has to install \textit{Python} environment at
least in version 2.6. Because implemented algorithms have many setting parameters, 
they were written to the file so that easily change their value in testing procedure. 
Output results(efficiency indicators) are written to \textsc{csv} for further
processing. The most preferable environment for running this project is
Eclipse, free to download from the Internet. Each classifier is written in the
form of \textit{Python} class so that further extension would be very easy. An
example of classifiers usage is presented in appendix \ref{Appendix}.
